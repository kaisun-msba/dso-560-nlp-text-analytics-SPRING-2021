{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "sun_kai_HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaisun-msba/dso-560-nlp-text-analytics-SPRING-2021/blob/main/sun_kai_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cutting-slovak"
      },
      "source": [
        "## Homework 2"
      ],
      "id": "cutting-slovak"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cutting-tiger"
      },
      "source": [
        "A. Using the **McDonalds Yelp Review CSV file**, **process the reviews**.\n",
        "This means you should think briefly about:\n",
        "* what stopwords to remove (should you add any custom stopwords to the set? Remove any stopwords?)\n",
        "* what regex cleaning you may need to perform (for example, are there different ways of saying `hamburger` that you need to account for?)\n",
        "* stemming/lemmatization (explain in your notebook why you used stemming versus lemmatization). \n",
        "\n",
        "Next, **count-vectorize the dataset**. Use the **`sklearn.feature_extraction.text.CountVectorizer`** examples from `Linear Algebra, Distance and Similarity (Completed).ipynb` and `Text Preprocessing Techniques (Completed).ipynb` (read the last section, `Vectorization Techniques`).\n",
        "\n",
        "I do not want redundant features - for instance, I do not want `hamburgers` and `hamburger` to be two distinct columns in your document-term matrix. Therefore, I'll be taking a look to make sure you've properly performed your cleaning, stopword removal, etc. to reduce the number of dimensions in your dataset. \n"
      ],
      "id": "cutting-tiger"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nonprofit-allocation"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('mcdonalds-yelp-negative-reviews.csv',encoding='latin1')\n",
        "df.head()"
      ],
      "id": "nonprofit-allocation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Eow7xtHcUt4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # A popular NLTK sentence tokenizer\n",
        "nltk.download('stopwords') # library of common English stopwords"
      ],
      "id": "4Eow7xtHcUt4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9M9ubo1CaRa"
      },
      "source": [
        "# stemming before count vectorization\n",
        "# better coverage than lemmatization\n",
        "# can reduce dimensions significantly\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer=PorterStemmer()\n",
        "df['tokenized_review']=df['review'].apply(lambda x:nltk.word_tokenize(x))\n",
        "df['stemmed_review']=df['tokenized_review'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
        "df['joined_review']=[' '.join(map(str, l)) for l in df['stemmed_review']]\n",
        "\n",
        "df.head()"
      ],
      "id": "I9M9ubo1CaRa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4nxeFTR0lO_"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk_stopwords = list(stopwords.words('english'))"
      ],
      "id": "k4nxeFTR0lO_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9NG4FXfxouv"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# specific stopwords to the mcdonald reviews\n",
        "add_stopwords=['mcdonald','mcdonalds','order','food','restaurant']\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words=nltk_stopwords+add_stopwords, token_pattern=r'\\b[a-zA-Z]{3,}\\b', min_df=0.05, max_df=0.4)\n",
        "X = vectorizer.fit_transform(df['joined_review'])\n",
        "vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
        "print(f\"Shape of dataframe is {vectorized_df.shape}\")\n",
        "print(f\"Total number of occurences: {vectorized_df.sum().sum()}\")\n",
        "#print(f\"Word counts: {vectorized_df.sum()}\")\n",
        "vectorized_df.head()"
      ],
      "id": "d9NG4FXfxouv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVDm6WMS8XYf"
      },
      "source": [
        "B. Stopwords, Stemming, Lemmatization Practice\n",
        "\n",
        "Using the tale-of-two-cities.txt file from Week 1:\n",
        "\n",
        "Count-vectorize the corpus. Treat each sentence as a document.\n",
        "How many features (dimensions) do you get when you:\n",
        "\n",
        "Perform stemming and then count-vectorization.\n",
        "Perform lemmatization and then count-vectorization.\n",
        "Perform lemmatization, remove stopwords, remove punctuation, and then perform count-vectorization?"
      ],
      "id": "AVDm6WMS8XYf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYQDxQjC-dKz"
      },
      "source": [
        "with open(\"tale-of-two-cities.txt\", \"r\") as text_file:\n",
        "  lines=text_file.read().replace(\"\\n\", \" \")\n",
        "lines"
      ],
      "id": "pYQDxQjC-dKz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArKuFnqttGVv"
      },
      "source": [
        "# tokenize the text into sentences\n",
        "lines=nltk.sent_tokenize(lines)\n",
        "len(lines)"
      ],
      "id": "ArKuFnqttGVv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N1jMdOsADUy"
      },
      "source": [
        "# 1.Perform stemming and then count-vectorization\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer=PorterStemmer()\n",
        "documents=[]\n",
        "for line in lines:\n",
        "  if len(line)>0:\n",
        "    line=[stemmer.stem(word) for word in nltk.word_tokenize(line)]\n",
        "    documents.append(line)\n",
        "documents=[' '.join(word_list) for word_list in documents]\n",
        "len(documents)"
      ],
      "id": "0N1jMdOsADUy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucbvNGUbCaiV"
      },
      "source": [
        "# countVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#roman_nums=['xii',\t'xiii',\t'xiv',\t'xix',\t'xvi',\t'xvii',\t'xviii',\t'xxi',\t'xxii',\t'xxiv','xxiii']\n",
        "#stop_words = set(stopwords.words('english') + roman_nums)\n",
        "vectorizer_1 = CountVectorizer()\n",
        "X_1 = vectorizer_1.fit_transform(documents)"
      ],
      "id": "ucbvNGUbCaiV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db2v4OAvPCGn"
      },
      "source": [
        "vectorized_df_1 = pd.DataFrame(X_1.toarray(), columns=vectorizer_1.get_feature_names())\n",
        "print(f\"Shape of dataframe is {vectorized_df_1.shape}\")\n",
        "print(f\"Total number of occurences: {vectorized_df_1.sum().sum()}\")\n",
        "vectorized_df_1"
      ],
      "id": "Db2v4OAvPCGn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkVYejLgMnsM"
      },
      "source": [
        "# 2.Perform lemmatization and then count-vectorization.\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "documents=[]\n",
        "for line in lines:\n",
        "  if len(line)>0:\n",
        "    line=[lemmatizer.lemmatize(word) for word in nltk.word_tokenize(line)]\n",
        "    documents.append(line)\n",
        "documents=[' '.join(word_list) for word_list in documents]"
      ],
      "id": "UkVYejLgMnsM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "939as_aLMnxx"
      },
      "source": [
        "# countVectorizer\n",
        "#roman_nums=['xii',\t'xiii',\t'xiv',\t'xix',\t'xvi',\t'xvii',\t'xviii',\t'xxi',\t'xxii',\t'xxiv','xxiii']\n",
        "# stop_words = set(stopwords.words('english') + roman_nums)\n",
        "vectorizer_2 = CountVectorizer()\n",
        "X_2 = vectorizer_2.fit_transform(documents)\n",
        "vectorized_df_2 = pd.DataFrame(X_2.toarray(), columns=vectorizer_2.get_feature_names())\n",
        "print(f\"Shape of dataframe is {vectorized_df_2.shape}\")\n",
        "print(f\"Total number of occurences: {vectorized_df_2.sum().sum()}\")\n",
        "vectorized_df_2"
      ],
      "id": "939as_aLMnxx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4GVdn822YDQ"
      },
      "source": [
        "# 3.Perform lemmatization, remove stopwords, remove punctuation, and then perform count-vectorization?\n",
        "# countVectorizer, removing stopwords\n",
        "roman_nums=['xii',\t'xiii',\t'xiv',\t'xix',\t'xvi',\t'xvii',\t'xviii',\t'xxi',\t'xxii',\t'xxiv','xxiii']\n",
        "stop_words = set(stopwords.words('english') + roman_nums)\n",
        "vectorizer_3 = CountVectorizer(stop_words=stop_words, token_pattern=r'\\b[a-zA-Z]{3,}\\b',)\n",
        "X_3 = vectorizer_3.fit_transform(documents)\n",
        "vectorized_df_3 = pd.DataFrame(X_3.toarray(), columns=vectorizer_3.get_feature_names())\n",
        "print(f\"Shape of dataframe is {vectorized_df_3.shape}\")\n",
        "print(f\"Total number of occurences: {vectorized_df_3.sum().sum()}\")\n",
        "vectorized_df_3"
      ],
      "id": "w4GVdn822YDQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CICuKa1S3gzY"
      },
      "source": [
        "# the third method results in less features comparing to the first two"
      ],
      "id": "CICuKa1S3gzY",
      "execution_count": null,
      "outputs": []
    }
  ]
}